# -*- coding: utf-8 -*-
"""TFG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XG52qUIpn0sLxuJs9NWBO_lEPck6nitj
"""

#Montamos y habilitamos acceso a nuestro Google Drive
from google.colab import drive
drive.mount('/content/gdrive',force_remount=True)

#Preparación de los datos en el nodo para trabajar localmente
!rm -r /tmp/*
!mkdir /home/yo/
!mkdir /tmp/clean/
!cp /content/gdrive/MyDrive/Colab_Notebooks/FrameWork_Colab/DDBB/clean.tar.xz /home/yo
!tar xvJf /home/yo/clean.tar.xz -C /home/yo
!mv /home/yo/test/ /tmp/clean/
!mv /home/yo/valid/ /tmp/clean/
!mv /home/yo/train/ /tmp/clean/

#DEFINICION DE NUESTRA CLASE DATASET -- Más simple porque no consdieramos ruidos, solo archivos limpios
from torch.utils.data import Dataset


#Librerias para tareas especificas
from scipy.io import wavfile  #Lectura de archivos WAV
import glob                   #Gestión de archivos
import os                     #Funciones del Sistema Operativo
import pickle                 #Lectura y salvado de objetos en disco




#Creamos nuestra clase
class CleanDataset(Dataset):
  # Tengo que particularizar la clase Dataset a mis necesidades. En este caso, no hay archivos ruidosos por lo que
  # modifico el Dataset de la red neuronal de realce de voz para adaptarla a archivos de audio limpios.

  def __init__(self,dir_clean,conj='train'):
    # Definimos la inicialización del con los parametros:
    #   dir_clean : (string) directorio donde se encuentra la base de datos a usar
    #   conj      : (string) subset de la BB.DD. (train,valid o set)

    self.dir_clean = dir_clean
    self.conj = conj

    self.clean = ['train','test','valid']

    # La parte en la que se nos definen los ruidos nos es indiferente puesto que estamos trabajando
    # con archivos de audio limpios. Pasamos directamente a la lectura y organización de los ficheros
    # de audio clean en un diccionario
    #self.sample_clean = self._get_dict_clean()

    # ESTA PARTE NO SE SI ES CORRECTA --> Como en el sistema de realce de voz se trabaja con archivos de
    # audio "contaminados" por ruido. Estos archivos vienen dados por aquellos que tienen una extensión
    # ".pkl" y se obtiene un listado de los mismos. Como en este caso solo trabajamos con archivos de audio
    # limpios cogeré aquellos de extensión ".wav"
    self.lista_files = glob.glob(self.dir_clean + self.conj + '/*.wav')
    # Con esta línea de código lo que estamos haciendo es crear un atributo llamado "lista_files" en la instancia
    # actual del objeto. Con el comando glob.glob devolvemos una lista de las rutas que coinciden con el patron
    # especificado --> Todos los archivos .wav del directorio especificado.

  def __len__(self):
    # Número total de ficheros del subset
    return len(self.lista_files)

  def __getitem__(self,idx):
    # Nos devuelve el sample identificado por idx en la lista de ficheros del set
    file_meta = self.lista_files[idx]
    example_id = os.path.splitext(os.path.basename(file_meta))[0]   # --> Con esta linea tomamos la ruta completa del archivo file_meta
                                                                    #     , extraemos su nombre y su extensión y luego elimina su extensión
                                                                    #     dejando solo el ID en la variable example_id

    # Leemos la señal de voz clean
    y = self._read_sample(file_meta[:-4] + '.wav')  # [:5*16000]


    # Construimos un "sample" de entrenamiento en forma de diccionario con los siguientes datos:
    sample = {'example_id': example_id,  'clean': y,  'seq_len': len(y)}


    return sample

  def _read_sample(self, file_speech):
      # Lee el fichero WAV y devuelve un vector con sus muestras
      time_signal = wavfile.read(file_speech)[1] * 1.0
      return time_signal

##### CONSTRUCCIÓN DE NUESTRA (CLASE DE) RED NEURONAL #####
import numpy as np

# Las capas utilizadas para esta red neuronal no serán lineales sino que serán de otro tipo.
# En su mayoria capas convolucionales y convolucionales transpuesta. Las funciones de activación
# que se implementan son la LeakyReLU y la sigmoid

import torch
import torch.nn as nn
import torch.nn.functional as F





    ##########################

######## DEFINIMOS NUESTRA RED NEURONAL DE WM BASADA EN EL PAPER DE PAVLOVIC

    # Esta red neuronal en realidad son dos redes neuronales:
    #     1.-  Una red neuronal encargada de introducir las marcas de agua en nuestras
    #          muestras de audio clean
    #     2.-  Una segunda red neuronal, entrenada con los archivos de audio con marca
    #          de agua, utilizada para detectar y extraer las marcas de agua de los audios



class embedding_WM(nn.Module):
    def __init__(self):
        super(embedding_WM, self).__init__()



        # Definición de las capas convolucionales

        #self.convprev = nn.Conv2d(in_channels=2,out_channels=8,kernel_size=5,stride=1,padding=2)

        self.conv_0 = nn.Conv2d(in_channels=2, out_channels=16, kernel_size=5, stride=2, padding=2)
        #self.batch_norm_0 = nn.BatchNorm2d(16)

        self.leaky_relu = nn.LeakyReLU(0.2)
        self.conv_1 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=2, padding=2)
        self.batch_norm_1 = nn.BatchNorm2d(32)
        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2, padding=2)
        self.batch_norm_2 = nn.BatchNorm2d(64)
        self.conv_3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2)
        self.batch_norm_3 = nn.BatchNorm2d(128)
        self.conv_4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, stride=2, padding=2)
        self.batch_norm_4 = nn.BatchNorm2d(256)
        self.conv_5 = nn.Conv2d(in_channels=256 + 512 , out_channels=256, kernel_size=5, stride=1, padding=2)


        # Definición de las capas de convolución transpuesta
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        self.conv_transpose_1 = nn.ConvTranspose2d(in_channels=512 , out_channels=128, kernel_size=5, stride=2, padding=2, output_padding=1)
        self.batchup1 = nn.BatchNorm2d(128)
        self.conv_transpose_2 = nn.ConvTranspose2d(in_channels=256 , out_channels=64, kernel_size=5, stride=2, padding=2, output_padding=1)
        self.batchup2 = nn.BatchNorm2d(64)
        self.conv_transpose_3 = nn.ConvTranspose2d(in_channels=128 , out_channels=32, kernel_size=5, stride=2, padding=2,output_padding=1)
        self.batchup3 = nn.BatchNorm2d(32)
        self.conv_transpose_4 = nn.ConvTranspose2d(in_channels=64 , out_channels=16, kernel_size=5, stride=2, padding=2, output_padding=1)
        self.batchup4 = nn.BatchNorm2d(16)
        self.conv_transpose_5 = nn.ConvTranspose2d(in_channels=32 , out_channels=8, kernel_size=5, stride=2, padding=2, output_padding=1)
        self.batchup5 = nn.BatchNorm2d(8)
        self.conv_final_6 = nn.Conv2d(in_channels=8 , out_channels=2, kernel_size=5, stride=1, padding=2)

    # El último paso para definir la clase de red neuronal es definir el método "forward"
    # que nos permite expresar como se convierte la entrada en la salida de la red
    # El método forward va a ser directamente llamado por el optimizador de la red

    def forward(self, inputSTFT,inputMessage):
        # Propagación hacia adelante de las capas convolucionales
        #conv_prev = self.leaky_relu(self.convprev(inputSTFT))
        conv_0_out = self.leaky_relu(self.conv_0(inputSTFT))
        conv_1_out = self.leaky_relu(self.batch_norm_1(self.conv_1(conv_0_out)))
        conv_2_out = self.leaky_relu(self.batch_norm_2(self.conv_2(conv_1_out)))
        conv_3_out = self.leaky_relu(self.batch_norm_3(self.conv_3(conv_2_out)))
        conv_4_out = self.leaky_relu(self.batch_norm_4(self.conv_4(conv_3_out)))


        ajuste = inputSTFT.size(0)
        inputMessage = inputMessage[:ajuste]


        # Concatenación con inputMessage
        bottleneck = torch.cat((conv_4_out, inputMessage), dim=1)

        # Propagación hacia adelante de las capas de convolución transpuesta
        conv_5_out = self.relu(self.conv_5(bottleneck))
        conv_transpose_1_out = self.relu(self.dropout(self.batchup1(self.conv_transpose_1(torch.cat((conv_5_out, conv_4_out), dim=1)))))
        conv_transpose_2_out = self.relu(self.dropout(self.batchup2(self.conv_transpose_2(torch.cat((conv_transpose_1_out, conv_3_out), dim=1)))))  # Concatenación de la dimensión de los canales
        conv_transpose_3_out = self.relu(self.dropout(self.batchup3(self.conv_transpose_3(torch.cat((conv_transpose_2_out, conv_2_out), dim=1)))))   # Concatenación de la dimensión de los canales
        conv_transpose_4_out = self.relu(self.dropout(self.batchup4(self.conv_transpose_4(torch.cat((conv_transpose_3_out, conv_1_out), dim=1)))))   # Concatenación de la dimensión de los canales
        conv_transpose_5_out = self.relu(self.dropout(self.batchup5(self.conv_transpose_5(torch.cat((conv_transpose_4_out, conv_0_out), dim=1)))))   # Concatenación de la dimensión de los canales
        conv_transpose_6_out = self.relu(self.conv_final_6(conv_transpose_5_out))


        return conv_transpose_6_out








# Ahora vamos a implementar la DNN que se corresponde con el detector de la WM (marca de agua). Para
# ello, de nuevo nos basamos en el paper de Pavlovic, en este caso en el diagrama de bloques de la
# figura4 donde el primer bloque representa la señal de entrada

# De la misma forma, definimos nuestra clase de DNN encargada de detectar las marcas de agua embebidasC
# en las señales de audio clean--> ¿qué BBDD utilizamos para entrenar esta DNN?¿TIMIT Clean o una BBDD
# generada a partir de la TIMIT Clean con WM embebidas?

class detector_WM(nn.Module):

  def __init__(self):
    # De nuevo llamamos al INIT de la superclase (padre), donde establecíamos los atributos
    # orientados al procesado de voz
    super(detector_WM, self).__init__()

    # De igual forma que para la DNN que se encarga de poner las WM *creo que* tenemos que declarar
    # un contexto inicial para los datos de entrada --> PREGUNTAR

    #···············································#
    ################## CONTEXTO #####################
    #···············································#

    # Definimos las capas de la DNN detectora de marcas de agua.

    # 1º CAPA
    self.conv1 = nn.Conv2d(in_channels=2, out_channels=32, kernel_size=5, stride=2, padding=2)
    self.bn1 = nn.BatchNorm2d(32)
    self.leaky_ReLU_1 = nn.LeakyReLU(0.2)

    # 2º CAPA
    self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=2, padding=2)
    self.bn2 = nn.BatchNorm2d(32)
    self.leaky_ReLU_2 = nn.LeakyReLU(0.2)

    # 3º CAPA
    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=(1,2), padding=2)
    self.bn3 = nn.BatchNorm2d(64)
    self.leaky_ReLU_3 = nn.LeakyReLU(0.2)

    # 4º CAPA
    self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, stride=(1,2), padding=2)
    self.bn4 = nn.BatchNorm2d(64)
    self.leaky_ReLU_4 = nn.LeakyReLU(0.2)

    # 5º CAPA
    self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=(1,2), padding=2)
    self.bn5 = nn.BatchNorm2d(128)
    self.leaky_ReLU_5 = nn.LeakyReLU(0.2)

    # 6º CAPA
    self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=5, stride=(1,2), padding=2)
    self.bn6 = nn.BatchNorm2d(128)
    self.leaky_ReLU_6 = nn.LeakyReLU(0.2)

    # ULTIMA CAPA
    self.flatten = nn.Flatten()

    self.fc = nn.Linear(128*128*1, 512) # 512 es el NUM_BITS

    self.Sigmoid = nn.Sigmoid()


  def forward(self,x):

    x_1 = self.leaky_ReLU_1(self.bn1(self.conv1(x)))
    x_2 = self.leaky_ReLU_2(self.bn2(self.conv2(x_1)))
    x_3 = self.leaky_ReLU_3(self.bn3(self.conv3(x_2)))
    x_4 = self.leaky_ReLU_4(self.bn4(self.conv4(x_3)))
    x_5 = self.leaky_ReLU_5(self.bn5(self.conv5(x_4)))
    x_6 = self.leaky_ReLU_6(self.bn6(self.conv6(x_5)))
    x_7 = self.flatten(x_6)
    x_8 = self.fc(x_7)
    x_final = self.Sigmoid(x_8)

    return x_final

# CUADRO DE FUNCIONES NECESARIAS PARA EL FUNCIONAMIENTO DE LAS REDES NEURONALES. EN ESTE CUADRO
# SE INCLUYEN LAS FUNCIONES DE COLLATE QUE DETERMINA COMO SE COGEN LAS SEÑALES PARA ENTRENAR LAS
# DNN Y LOS ATAQUES IMPLEMENTADOS A LA SALIDA DEL AUTOENCODER.

# Deficnición de las bibliotecas

import torch
import torch.nn as nn
import numpy as np
from scipy.signal import butter, filtfilt
from random import randint
import sys

#MODULE_FULL_PATH = '/content/gdrive/MyDrive/MISCOLABS/Bibliotecas'
#sys.path.append('/content/gdrive/MyDrive/MISCOLABS/Bibliotecas')

#from auxiliares import stft, hann_sqrt

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# En primer lugar, tenemos la función custom_collate_fn que nos permite manejar los lotes de audio.
# Toma los primeros 2.044 secs de cada audio, lo convierte a tensor y calcula su STFT

def custom_collate_fn(batch):
    """
    Una función de colación personalizada para manejar lotes de audios.
    """

    desired_length = 511*64

    audio_batch  = torch.stack([torch.from_numpy(sample['clean'][:desired_length]) for sample in batch])


    batch_stft = torch.stft(audio_batch, n_fft = num_fft, hop_length=511,window = window, normalized = True,return_complex = True) # Tensor de la forma [Frecuencia, Tiempo, Complejo]

    # Reajustamos las dimensiones para que cuadren con la entrada --> inputSTFT (64,2,512,64)
    # Ahora tenemos un tensor que tiene unicamente 3 dimensiones. Queremos añadir la dimensión de canal para que en un canal
    # vaya la parte real del tensor y en el otro canal vaya la parte imaginaria
    batch_stft_real = batch_stft.real.unsqueeze(1)    # Cogemos la parte real y añadimos la dimension de canal
    batch_stft_imag = batch_stft.imag.unsqueeze(1)    # Con la parte imaginaria hacemos lo mismo

    # Por último, concatenamos a lo largo de la dimensión de canales

    batch = torch.cat((batch_stft_real,batch_stft_imag),dim=1).float() # Lo pasamos a float para que el tipo de dato coincida con el bias

    # Devuelve el lote de audios rellenados
    return batch




def custom_collate_fn_test(batch):
    """
    Una función de colación personalizada para manejar lotes de audios,
    concatenando las STFTs de bloques de 2 segundos para formar la STFT de la señal completa.
    """

    batch_output = []  # Lista para almacenar las STFTs concatenadas de cada muestra

    for sample in batch:
        audio = torch.from_numpy(sample['clean'])  # Convierte la muestra de audio a tensor
        total_length = audio.shape[0]  # Longitud total de la muestra de audio
        segment_length = 511 * 64  # Longitud de cada segmento de audio (2 segundos)

        # Calcula el número de segmentos necesarios para cubrir la señal completa
        num_segments = int(np.ceil(total_length / segment_length))

        # Lista para almacenar las STFTs de cada segmento
        segments_stft = []

        for i in range(num_segments):
            # Determina el inicio y el fin del segmento actual
            start = i * segment_length
            end = start + segment_length
            segment = audio[start:end]

            # Añade padding al último segmento si es necesario
            if segment.shape[0] < segment_length:
                padding_size = segment_length - segment.shape[0]
                segment = F.pad(segment, (0, padding_size))

            # Calcula la STFT para el segmento actual
            segment_stft = torch.stft(segment, n_fft=num_fft, hop_length=511, window=window, normalized=True, return_complex=True)

            # Separa las partes real e imaginaria, y las prepara para la concatenación
            segment_stft_real = segment_stft.real.unsqueeze(1)  # Parte real
            segment_stft_imag = segment_stft.imag.unsqueeze(1)  # Parte imaginaria
            segment_stft_combined = torch.cat((segment_stft_real, segment_stft_imag), dim=1).float()  # Concatena a lo largo de la dimensión de canal
            segment_stft_combined = segment_stft_combined.unsqueeze(0)
            segments_stft.append(segment_stft_combined)

        # Concatena todas las STFTs de los segmentos para formar la STFT completa de la muestra

        audio_stft_full = torch.cat(segments_stft, dim=0)  # Concatena a lo largo de la dimensión temporal

        batch_output.append(audio_stft_full)

    # Al final, `batch_output` es una lista de tensores, cada uno representando la STFT completa de una muestra de audio.
    # Nota: Los tensores pueden tener diferentes longitudes temporales debido a la variabilidad en la longitud de las señales de audio originales.
    return batch_output




# ATAQUES APLICADOS A LA STFT ANTES DE PASAR AL DECODER COMO CAPAS NO ENTRENABLES
# DE LA DNN:
#           1.- Low-pass Filter con frecuencia de corte fc = 4 kHz.
#           2.- Adición de Ruido aleatorio Gaussiano
#           3.- Sample Supression que pone un número aleatorio de muestras de
#               la señal a 0.

# LOW PASS FILTER BUTTERWORTH CON fc = 4 kHz

def butter_lowpass_filter(signal,cutoff=4000,fs=16000,order=16):
    nyq= 0.5 * fs
    normal_cutoff = cutoff/nyq
    b,a = butter(order,normal_cutoff,btype='low',analog=False)
    y = filtfilt(b,a,signal)
    return y


# ATAQUE FILTRO BUTTER PASO-BAJA SOBRE LAS SEÑALES QUE SALEN DEL AUTOENCODER COMO
# CAPA NO ENTRENABLE DE LA RED NEURONAL

class LowPassFilter(nn.Module):
  def __init__(self):
    super(LowPassFilter,self).__init__()

  def forward(self,inputs,coeficiente):
    # Generamos un coeficiente de decisión para
    # aplicar o no el ataque de filtro paso baja
    coef_dec = randint(1,101)

    if coef_dec <= coeficiente :
      print("LoWPassFilter")
      # Las señales de entrada son un tensor de dimensiones [64,2,512,64]=[batch_size,channels
      # freq_bins,frames]. Dado que los canales representan las partes real e imaginaria de la
      # STFT representada como un tensor, procedemos la descomposición:

      signal_r = inputs[:,0,:,:]
      signal_i = inputs[:,1,:,:]

      signal_complex = torch.complex(signal_r,signal_i)

      # Calculamos la ISTFT con los mismos parametros y obtenemos las señales de audio
      signal_audio = torch.istft(signal_complex,n_fft=1023,hop_length=511,win_length=1023,window=torch.sqrt(torch.hann_window(1023)).to(device),normalized=True)

      # Inicializamos un vector vacío que nos guarde las señales filtradas
      filtered_signals = []


      # Aplicamos LowPass filter a cada muestra que conforma el batch
      for i in range(inputs.shape[0]):
          # Extraemos muestra i-ésima
          sample = signal_audio[i].detach().cpu().numpy() # Pasamos a la CPU

          # Filtramos la muestra i-ésima
          filtrado = butter_lowpass_filter(sample)

          # Convertimos resultado a tensor y lo agregamos a la lista
          filt_tensor = torch.tensor(filtrado.copy(),dtype=torch.float32,device=device)
          filtered_signals.append(filt_tensor)

      # Pasamos la lista de señales filtradas a tensor
      filtered_signals = torch.stack(filtered_signals)

      # Realizamos la STFT y volvemos a reconstruir el tensor [64,2,512,64]
      resultado = torch.stft(filtered_signals,n_fft=1023,hop_length=511,win_length=1023,window=torch.sqrt(torch.hann_window(1023)).to(device),normalized=True,return_complex=True)

      resultado_real = resultado.real.unsqueeze(1)
      resultado_imag = resultado.imag.unsqueeze(1)

      resultado_final = torch.cat((resultado_real,resultado_imag),dim=1).float()

      return resultado_final

    else:
      return inputs

# ATAQUE DE RUIDO ADITIVO COMO CAPA NO ENTRENABLE DE LAS REDES NEURONALES
class AdditiveNoise(nn.Module):
  def __init__(self):
    super(AdditiveNoise,self).__init__()

  def forward(self,inputs,coeficiente,noise_stregth):
    # Generamos un coeficiente de decisión para
    # aplicar o no el ataque de ruido aditivo
    coef_dec = randint(1,101)

    if coef_dec <= coeficiente:
      print("Additive Noise")

      # Las señales de entrada son un tensor de dimensiones [64,2,512,64]=[batch_size,channels
      # freq_bins,frames]. Dado que los canales representan las partes real e imaginaria de la
      # STFT representada como un tensor, procedemos la descomposición:

      signal_r = inputs[:,0,:,:]
      signal_i = inputs[:,1,:,:]

      signal_complex = torch.complex(signal_r,signal_i)

      # Calculamos la ISTFT con los mismos parametros y obtenemos las señales de audio
      signal_audio = torch.istft(signal_complex,n_fft=1023,hop_length=511,win_length=1023,window=torch.sqrt(torch.hann_window(1023)).to(device),normalized=True)

      # Generamos ruido de las mismas dimensiones que la signal_audio
      ruido = torch.rand(signal_audio.shape,device=device) * noise_strength
      noisy_signal = signal_audio + ruido

      # Calcula la STFT de la señal ruidosa
      signal_stft = torch.stft(noisy_signal,n_fft=1023,hop_length=511,win_length=1023,window=torch.sqrt(torch.hann_window(1023)).to(device), normalized=True, return_complex=True)

      # Hacemos la misma operación que en la función anterior para obtener un tensor que represente a la STFT
      # separada en dos canales: uno para la parte real y otro para la parte imaginaria.
      resultado_real = signal_stft.real.unsqueeze(1)
      resultado_imag = signal_stft.imag.unsqueeze(1)

      resultado_final = torch.cat((resultado_real,resultado_imag),dim=1).float()

      return resultado_final

    else:
      return inputs


# ATAQUE DE SUPRESION DE MUESTRAS COMO CAPA NO ENTRENABLE DE LA RED NEURONAL
class SampleSupression(nn.Module):
  def __init__(self):
    super(SampleSupression,self).__init__()

  def forward(self,inputs,coeficiente,batch_size,num_samples):
    # Generamos un coeficiente de decisión para
    # aplicar o no el ataque de supresión de muestras
    coef_dec = randint(1,101)

    if coef_dec <= coeficiente :
      print("Sample Supression")

      # Hacemos la misma operación que en los dos ataques anteriores para separar
      # parte real e imaginaria del tensor y unirla en un mismo tensor para poder
      # calcular la ISTFT ya que esta requiere tensores complejos a la entrada

      signal_r = inputs[:,0,:,:]
      signal_i = inputs[:,1,:,:]

      signal_complex = torch.complex(signal_r,signal_i)
      signal_audio = torch.istft(signal_complex,n_fft=1023,hop_length=511,win_length=1023,window=torch.sqrt(torch.hann_window(1023)).to(device),normalized=True)

      # Tenemos que expandir la dimensión de signal_audio para mantener la compatibilidad
      # de la estructura con tensorflow

      signal_audio = signal_audio.unsqueeze(-1)


      # Recortamos las muestras aleatoriamente

      for i in range(batch_size):
          indices_to_cut = torch.randint(0,signal_audio.shape[1]-1,(num_samples,))
          signal_audio[i,indices_to_cut,:] = 0 # Ponemos a 0 los indices seleccionados

      # Convertimos de nuevo las señales en el DT al DF a través de la STFT. Primero eliminamos
      # la dimensión añadida para no romper la compatibilidad de la estructura con tensor_flow

      signal_audio = signal_audio.squeeze(-1)
      signal_stft = torch.stft(signal_audio,n_fft=1023,hop_length=511,win_length=1023,window=torch.sqrt(torch.hann_window(1023)).to(device), normalized=True, return_complex=True)

      # Hacemos la misma operación que en la función anterior para obtener un tensor que represente a la STFT
      # separada en dos canales: uno para la parte real y otro para la parte imaginaria.
      resultado_real = signal_stft.real.unsqueeze(1)
      resultado_imag = signal_stft.imag.unsqueeze(1)

      resultado_final = torch.cat((resultado_real,resultado_imag),dim=1).float()

      return resultado_final

    else:
      return inputs

# FUNCIONES QUE DEFINEN LOS PESOS DE LAS DNN

def set_weights(step):
  global encoder_w, decoder_w,counter

  if step>0 and step%166==0:
    counter += 1
    if counter == 9 :
      counter = 0
     # Si el step es mayor que 0 o multimo de 166 modificamos los pesos
      if step < 14000:
         encoder_w += 0.2
         decoder_w -= 0.2
  # Si el step es mayor que 14000 fijamos los pesos
  if step >= 14000:
      encoder_w = 2.5
      decoder_w = 0.5

def set_weights_v(step_v):
  global encoder_w_v, decoder_w_v,counter_v
  if step_v > 0 and step_v%339==0:
    counter_v +=1
    if counter_v == 4:
      counter_v = 0
      if step_v < 28000:
         encoder_w_v += 0.2
         decoder_w_v -= 0.2
  # Si el step es mayor que 28000 fijamos los pesos
  if step_v >= 28000:
    encoder_w_v = 2.5
    decoder_w_v = 0.5

# Función de UMBRALIZACION

def umbralizacion(tensor,umbral):
  tensor_umb = (tensor > umbral).float()
  tensor_umb = torch.tensor(tensor_umb,requires_grad=True)
  return tensor_umb


# Funcion para ajustar los pesos de la MAE loss y de la PESQ loss

def train_weigths(epoch,num_epochs,transition_epoch):
  global mae_w, pesq_w

  if epoch <= transition_epoch:
    mae_w = 1
    pesq_w = 0

  elif epoch > transition_epoch and epoch <= 40:
    mae_w -= 0.01
    pesq_w += 0.01

  elif epoch > 140:
    mae_w = 0.06
    pesq_w = 1

  pesq_ws.append(pesq_w)
  mae_ws.append(mae_w)
  epochs.append(epoch)

def train_weigths_v(epoch,num_epochs,transition_epoch):
  global mae_w_v, pesq_w_v

  if epoch < transition_epoch:
    mae_w_v = mae_w_v
    pesq_w_v = pesq_w_v

  elif epoch > transition_epoch and epoch <= 40:
    mae_w_v -= 0.01
    pesq_w_v += 0.01

  elif epoch > 140:
    mae_w_v = 0.06
    pesq_w_v = 1

  pesq_ws_v.append(pesq_w_v)
  mae_ws_v.append(mae_w_v)
  epochs.append(epoch)


def snr(input_signal, output_signal):
    Ps = np.sum(np.abs(input_signal ** 2))
    Pn = np.sum(np.abs((input_signal-output_signal) ** 2))
    return 10 * np.log10((Ps/Pn))

#### OPTIMIZADOR #### --> ¿CUÁLES SON LOS PARAMETROS QUE NOSOTROS VAMOS A UTILIZAR?¿LOS MISMOS QUE LA DNN DE REALCE DE VOZ?

# Importamos la biblioteca de optimizadores de Pytorch

import torch.optim as optim

# El optimizador se aplica sobre una lista de tensores que queremos optimizar, que en nuestro caso, compondrá los parámetros
# de nuestra red neuronal. Por tanto, vamos a instanciar nuestra clase de DNN generando un objeto (la DNN que vamos a entrenar)

# En primer lugar establecemos las opciones de nuestro experimento. Generalmente esto se implementa mediante parámetros en la
# línea de ordenes al llamar al script de python , empleando la librearia 'util.parse_args' para procesarlos. Esta librería nos
# permite generar directamente el objeto

window_length = 1023       # Tamaño de la ventana (muestras) para analisis DFT en tiempo corto
dropout_rate = 0.5         # Tasa de dropout para controlar el overfitting
early_stop = 100             # Paciencia de la red (número de épocas) frente a no mejora en el conjunto de validación
batch_size = 10
num_bits = 512
fs = 16000                 # Frecuencia de muestreo
num_fft = 1023             # Número de samples para la STFT
window = torch.sqrt(torch.hann_window(window_length)) # Ventana de Hanning utilizada para la STFT
coeficiente = 30           # Fijamos la probabilidad con la que ocurren los ataques. Coeficiente=15 indica 15% de prob de ataque
noise_strength = 16.0       # Fuerza del ruido de ataque en el ataque de adición de ruido
num_samples = 1000         # Número de samples que se eliminan en el ataque de supresión de muestras
hop_length = 511
encoder_w = 1.0            # Peso inicial del encoder
decoder_w = 3.0            # Peso inicial del decoder
encoder_w_v = 1.0          # Peso inicial del encoder en el valid
decoder_w_v = 3.0          # Peso inicial del decoder en el valid

pesq_w = 0                 # Peso inicial de la PESQ loss
mae_w = 1                  # Peso inicial del MAE loss

pesq_w_v = 0              # Peso inicial de la PESQ loss en el valid
mae_w_v = 1                  # Peso inicial del MAE loss en el valid

counter = 0                # Variable contadora del train
counter_v = 0              # Variable contadora del valid

# Creo vectores para los pesos
num_epochs = 200
pesq_ws = []
mae_ws = []
pesq_ws_v = []
mae_ws_v = []
epochs = []

for epoch in range(num_epochs):
    train_weigths(epoch, num_epochs, 120)
    train_weigths_v(epoch,num_epochs,120)




# Instanciamos el Autoencoder y el Detector y los ataques
estimator = embedding_WM()
estimator_2 = detector_WM()
lowpass_attack = LowPassFilter()
additivenoise_attack = AdditiveNoise()
samplesupresion_attack = SampleSupression()

# Construimos finalmente el optimizador
parametros_DNN = list(estimator.parameters()) + list(estimator_2.parameters())


optimizer = optim.Adam(parametros_DNN,lr=0.0001)

#### ENTRENAMIENTO DE LA RED NEURONAL #### ---> ¿Necesitamos dos entrenamientos o podemos hacerlo todo en uno?

#### 1.- DATALOADERS

# Lo primero que tenemos que hacer es preparar el DataLoader. Este tipo de objetos nos permiten paralelizar ya que
# mientras que la GPU se encarga de ejecutar los forward y backward step de entrenamiento sobre el batch actual, la CPU
# realiza la tarea de leer y procesar los datos del siguiente batch

# Pytorch se va a encargar de todo el trabajo sucio. Hay que pasar el dataset compatible al DataLoader (como definimos eN
# nuestra clase CleanDataset) y un método de construcción de batches compatibles con la red a partir de samples como el que
# está definido en nuestra DNN de procesado de voz

from torch.utils.data import DataLoader

# Definimos Datasets de entrenamiento y validacion

clean_set_train = CleanDataset('/tmp/clean/', conj='train')
clean_set_valid = CleanDataset('/tmp/clean/', conj='valid')

# DataLoaders de entrenamiento y validación
# LLamamos al método 'collate_fn'...
train_dataloader = DataLoader(clean_set_train, batch_size=batch_size, shuffle=True, num_workers=5, collate_fn = custom_collate_fn)
valid_dataloader = DataLoader(clean_set_valid, batch_size=1, shuffle=False, num_workers=5, collate_fn = custom_collate_fn)

##### ENTRENAMIENTO DE LA RED NEURONAL ####
import numpy as np
import time
from random import randint
import sys



MODULE_FULL_PATH = '/content/gdrive/MyDrive/MISCOLABS/Bibliotecas'
sys.path.append('/content/gdrive/MyDrive/MISCOLABS/Bibliotecas')

from losses import pmsqeTime_loss
from pmsqe_torch import PMSQE
from sdr import sdr
from auxiliares import hann_sqrt,istft

#from pesq import pesq


#### 2.- Bucle de optimización



# Mandamos a la GPU si está disponible la DNN y la función de coste
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Funciones perdida encoder y decoder y mandamos a GPU
loss_function = nn.L1Loss()
pesq_encoder_loss = pmsqeTime_loss()
loss_function_WM = nn.BCELoss()
loss_function.to(device)
pesq_encoder_loss.to(device)
loss_function_WM.to(device)

# Mandamos los modelos a la GPU
estimator.to(device)
estimator_2.to(device)

# Mandamos a la GPU los ataques
lowpass_attack.to(device)
additivenoise_attack.to(device)
samplesupresion_attack.to(device)

# Definimos la marca de agua con las dimensiones que se nos especifican (16,2,512) --> tensor. Voy
# a definir una marca de agua fija para hacer las pruebas pertinentes. Va a ser 1s y 0s alternativos por ejemplo

mensaje = np.load('/content/gdrive/MyDrive/MISCOLABS/message_pool.npy')

# Comenzamos el entrenamiento

print("[*]Start training...")
num_batch_train = len(train_dataloader)
num_batch_valid = len(valid_dataloader)

num_epochs = 200
step = 0
step_v = 0
save_net = 'modelo_e_30_pesq_nv.pth.tar'
save_net_2 = 'modelo_d_30_pesq_nv.pth.tar'




strike = 0
#old_mean_val_distortion = 1e100
#old_mean_val_distortion_WM = 1e100

best_BER = 0.5
old_sdr = -30


# Para almacenar las pérdidas en cada epoch

loss_vector = np.zeros(num_epochs)
loss_vector_WM = np.zeros(num_epochs)
loss_vector_tot = np.zeros(num_epochs)
avg_loss_vec = np.zeros(num_epochs)
avg_loss_WM_vec = np.zeros(num_epochs)
avg_loss_tot_vec = np.zeros(num_epochs)
valid_loss_vec = np.zeros(num_epochs)
valid_loss_WM_vec = np.zeros(num_epochs)
valid_loss_tot_vec = np.zeros(num_epochs)

epoch_accuracy = np.zeros(num_epochs)
epoch_ber = np.zeros(num_epochs)

snr_vec = np.zeros(num_epochs)
sdr_vec = np.zeros(num_epochs)
ber_valid = np.zeros(num_epochs)
valid_acc = np.zeros(num_epochs)
snr_valid = np.zeros(num_epochs)
sdr_valid = np.zeros(num_epochs)

l_mae = np.zeros(num_epochs)
l_pesq = np.zeros(num_epochs)
l_tot = np.zeros(num_epochs)


# Bucle de entrenamiento
for epoch in range(num_epochs):

  # De la misma manera que en la DNN de realce de voz , las capas de dropout o batch normalization
  # se comportan de distinta forma si estamos entrenando la red o si estamos evaluándola. Para ello
  # aplicamos los métodos 'train' y 'eval' de la clase 'nn.Module'

  estimator.train()
  estimator_2.train()
  avg_loss = 0.0
  avg_loss_WM = 0.0
  avg_loss_tot = 0.0
  avg_SNR = 0.0
  avg_SDR = 0.0
  avg_PESQ = 0.0
  total = 0
  incorrect = 0
  start_time = time.time()

  for id,batch in enumerate(train_dataloader):

    # Enviamos al dispositivo. Transferimos los tensores de la CPU a la GPU
    # para realizar el entrenamiento
    inputSTFT = batch


    # Generamos la marca de agua de un pool de mensajes aleatorios
    ind = randint(0, len(mensaje)-1)
    msg = np.broadcast_to(mensaje[ind], (batch_size, num_bits))
    temp = np.empty((batch_size,16,2,num_bits))
    temp[:,:,:,:] = np.expand_dims(msg,axis=(1,2))
    input_Message = torch.tensor(temp).float()
    input_Message = input_Message.permute(0,3,1,2)


    # Pasamos los tensores a la GPU
    inputSTFT = inputSTFT.to(device)
    input_Message = input_Message.to(device)

    # El entrenamiento requiere BackPropagation por lo que es necesario que todos
    # los tensores activen sus gradientes. Para no tener que ir uno por uno activándolos
    # activamos un contexto en python:

    with torch.set_grad_enabled(True):

      # Reseteamos los gradientes a 0 (si no se acumularían los nuevos sobre los de la iteración anterior)
      optimizer.zero_grad()

      # Forward Pass
      output = estimator(inputSTFT,input_Message)
      output_atacada = lowpass_attack(output,coeficiente)
      output_atacada = additivenoise_attack(output_atacada,coeficiente,noise_strength)
      output_atacada = samplesupresion_attack(output_atacada,coeficiente,output.size(0),num_samples)
      WM_reconstructed = estimator_2(output_atacada)
      WM_reconstructed = WM_reconstructed.unsqueeze(-1).unsqueeze(-1)
      WM_reconstructed = WM_reconstructed.repeat(1,1,16,2)


      ajuste = WM_reconstructed.size(0)
      input_Message_adjust = input_Message[:ajuste]

      # Cálculo de pérdidas

      # Pasamos al dominio temporal para añadir la PESQ loss nueva
      entrada = torch.complex(inputSTFT[:,0,:,:],inputSTFT[:,1,:,:])
      salida = torch.complex(output[:,0,:,:],output[:,1,:,:])
      entrada_DT_ten = torch.istft(entrada,n_fft=num_fft,hop_length=511,win_length=1023,window=window.to(device),normalized=True)
      salida_DT_ten = torch.istft(salida,n_fft=num_fft,hop_length=511,win_length=1023,window=window.to(device),normalized=True)


      # Creamos el batch que contiene a ambos tensores
      loss_batch = {'target_time':entrada_DT_ten,
                    'output_time':salida_DT_ten}


      #print("PESQ Loss:",pesq_encoder_loss(loss_batch))

      loss_mae = loss_function(output,inputSTFT)
      loss_pesq = pesq_encoder_loss(loss_batch)



      loss = (loss_mae/60 + pesq_ws[epoch]*loss_pesq/30) * (60/2)

      #loss = loss_function(output,inputSTFT) + pesq_ws[epoch]*pesq_encoder_loss(loss_batch)
      loss_WM = loss_function_WM(WM_reconstructed[:,:,0,0],input_Message_adjust[:,:,0,0])
      # Cálculo de SNR y PESQ

      #entrada_DT = istft(entrada_np,hann_sqrt(1023,511),signal_length=511*64,size=window_length,shift=hop_length,fading=True)
      #salida_DT = istft(salida_np,hann_sqrt(1023,511),signal_length=511*64,size=window_length,shift=hop_length,fading=True)

      entrada_DT = entrada_DT_ten.cpu().detach().numpy()[0]
      salida_DT = salida_DT_ten.cpu().detach().numpy()[0]

      SDR = sdr(entrada_DT,salida_DT)
      SNR = snr(entrada_DT/max(np.abs(entrada_DT)),salida_DT/max(np.abs(salida_DT)))


      # Aplicamos los pesos para cada perdida de las dos redes neuronales. Estas
      # variarán en función del número de epoch en el que estemos

      set_weights(step)
      step += 1

      #factor_norm = 60.75/(60.75 + pesq_ws[epoch]*30)
      #print("Factor Norm =" , factor_norm)


      loss_tot = encoder_w*loss + decoder_w*loss_WM

      l_mae[epoch] += loss_mae.item() / num_batch_train
      l_pesq[epoch] += loss_pesq.item() / num_batch_train
      l_tot[epoch] += (loss_mae.item() + pesq_ws[epoch]*loss_pesq.item()) / num_batch_train

      avg_loss += loss.item() / num_batch_train
      avg_loss_WM += loss_WM.item() / num_batch_train
      avg_loss_tot += loss_tot.item() / num_batch_train

      avg_SDR  += SDR


      # Calculamos el accuracy del decoder
      WM_reconstructed = umbralizacion(WM_reconstructed,0.5)

      incorrect_predictions = (WM_reconstructed[:,:,0,0] != input_Message_adjust[:,:,0,0])
      ber = incorrect_predictions.float().mean().item()

      # Acumulación del accuracy para promediar luego
      incorrect += incorrect_predictions.float().sum().item()
      total += WM_reconstructed[:,:,0,0].nelement()

      # Backward Pass y Optimización
      loss_tot.backward()
      optimizer.step()



  # Imprimimos las pérdidas en cada epoch y el tiempo que tarda
  print_line = "Epoch: [%2d] time: %4.4f, loss_autoencoder: %.4f" % (epoch + 1, time.time() - start_time, avg_loss)
  print(print_line)
  loss_vector[epoch] = loss
  avg_loss_vec[epoch] = avg_loss

  print_line_WM = "Epoch: [%2d] time: %4.4f, loss_WM: %.4f" % (epoch + 1, time.time() - start_time, avg_loss_WM)
  print(print_line_WM)
  loss_vector_WM[epoch] = loss_WM
  avg_loss_WM_vec[epoch] = avg_loss_WM

  print_line_tot = "Epoch: [%2d] time: %4.4f, loss_total: %.4f" % (epoch + 1, time.time() - start_time, avg_loss_tot)
  print(print_line_tot)
  loss_vector_tot[epoch] = loss_tot
  avg_loss_tot_vec[epoch] = avg_loss_tot

  epoch_ber[epoch] = incorrect/total
  epoch_accuracy[epoch] = 1- epoch_ber[epoch]
  print_line_acc = "Epoch: [%2d] time: %4.4f, BER: %.4f, accuracy: %.4f" % (epoch + 1, time.time() - start_time, epoch_ber[epoch],epoch_accuracy[epoch])
  print(print_line_acc)

  snr_vec[epoch] = avg_SNR / num_batch_train
  print_line_SNR = "Epoch: [%2d] time: %4.4f, SNR: %.4f dB" % (epoch + 1, time.time() - start_time ,snr_vec[epoch])
  print(print_line_SNR)

  sdr_vec[epoch] = avg_SDR / num_batch_train
  print_line_SDR = "Epoch: [%2d] time: %4.4f, SNR: %.4f dB" % (epoch + 1, time.time() - start_time ,sdr_vec[epoch])
  print(print_line_SDR)

  print_line_w = "Epoch: [%2d] time: %4.4f, MAE_w: %.4f, PESQ_w: %.4f" %(epoch + 1, time.time() - start_time ,mae_ws[epoch],pesq_ws[epoch])
  print(print_line_w)

  # Acabado un epoch del bucle de entrenamiento procedemos a ver el rendimiento de la red
  # sobre el conjunto de evaluación. Ponemos las capas necesarias en modo 'evaluación'

  estimator.eval()
  estimator_2.eval()

  valid_loss = 0.0
  valid_loss_WM = 0.0
  valid_loss_tot = 0.0
  valid_SDR = 0.0
  valid_SNR = 0.0
  incorrect_valid = 0.0
  total_valid = 0.0
  start_time = time.time()

  for id,batch in enumerate(valid_dataloader):

    inputSTFT_valid = batch

    # Generamos la marca de agua de un pool de mensajes aleatorios
    ind = randint(0, len(mensaje)-1)
    msg = np.broadcast_to(mensaje[ind], (1, num_bits))
    temp = np.empty((1,16,2,num_bits))
    temp[:,:,:,:] = np.expand_dims(msg,axis=(1,2))
    input_Message_valid = torch.tensor(temp).float()
    input_Message_valid = input_Message_valid.permute(0,3,1,2)

    # Mandamos tensores a la GPU
    inputSTFT_valid = inputSTFT_valid.to(device)
    input_Message_valid = input_Message_valid.to(device)

    # Puesto que con estos datos no vamos a realizar la retropropagación, podemos deshabilitar
    # el cómputo de gradientes mejorando de esta forma el rendimiento

    with torch.no_grad():

      # Forward pass
      output_valid = estimator(inputSTFT_valid, input_Message_valid)
      output_atacada_valid = lowpass_attack(output_valid,coeficiente)
      output_atacada_valid = additivenoise_attack(output_atacada_valid,coeficiente,noise_strength)
      output_atacada_valid = samplesupresion_attack(output_atacada_valid,coeficiente,output_atacada_valid.size(0),num_samples)
      WM_reconstructed_valid = estimator_2(output_atacada_valid)
      WM_reconstructed_valid = WM_reconstructed_valid.unsqueeze(-1).unsqueeze(-1)
      WM_reconstructed_valid = WM_reconstructed_valid.repeat(1,1,16,2)

      ajuste_valid = WM_reconstructed_valid.size(0)
      input_Message_adjust_valid= input_Message_valid[:ajuste_valid]

      # Introducimos el loss_batch para la nueva loss PESQ
      entrada_valid = torch.complex(inputSTFT_valid[:,0,:,:],inputSTFT_valid[:,1,:,:])
      salida_valid = torch.complex(output_valid[:,0,:,:],output_valid[:,1,:,:])
      entrada_DT_valid_ten = torch.istft(entrada_valid,n_fft=num_fft,hop_length=511,win_length=1023,window=window.to(device),normalized=True)
      salida_DT_valid_ten = torch.istft(salida_valid,n_fft=num_fft,hop_length=511,win_length=1023,window=window.to(device),normalized=True)

      loss_batch_valid = {'target_time':entrada_DT_valid_ten,
                          'output_time':salida_DT_valid_ten}


      #print("PESQ Loss:",pesq_encoder_loss(loss_batch_valid))

      loss_mae_valid = loss_function(output_valid,inputSTFT_valid)
      loss_pesq_valid = pesq_encoder_loss(loss_batch_valid)

      loss_valid = (loss_mae_valid/55 + pesq_ws_v[epoch]*loss_pesq_valid/30)*(55/2)

      #loss_valid = loss_function(output_valid,inputSTFT_valid) + pesq_ws_v[epoch]*pesq_encoder_loss(loss_batch_valid)
      loss_WM_valid = loss_function_WM(WM_reconstructed_valid[:,:,0,0],input_Message_adjust_valid[:,:,0,0])

      set_weights_v(step_v)
      step_v += 1

      loss_tot_valid = encoder_w_v*loss_valid + decoder_w_v*loss_WM_valid

      # Cálculo de SNR y PESQ
      #entrada_DT_valid = istft(entrada_valid,np.sqrt(np.hanning(window_length)),signal_length=511*64,size=window_length,shift=hop_length,fading=True)
      #salida_DT_valid = istft(salida_valid,np.sqrt(np.hanning(window_length)),signal_length=(511*64),size=window_length,shift=hop_length,fading=True)
      entrada_DT_valid = entrada_DT_valid_ten.cpu().detach().numpy()
      salida_DT_valid = salida_DT_valid_ten.cpu().detach().numpy()

      SDR_valid = sdr(entrada_DT_valid,salida_DT_valid)
      SNR_valid = snr(entrada_DT_valid/max(np.abs(entrada_DT_valid)),salida_DT_valid/max(np.abs(salida_DT_valid)))


      # Cálculo del BER
      WM_reconstructed_valid = umbralizacion(WM_reconstructed_valid,0.5)

      incorrect_predictions_valid = (WM_reconstructed_valid[:,:,0,0] != input_Message_adjust_valid[:,:,0,0])
      ber = incorrect_predictions_valid.float().mean().item()

      # Acumulación del accuracy para promediar luego
      incorrect_valid += incorrect_predictions_valid.float().sum().item()
      total_valid += WM_reconstructed_valid[:,:,0,0].nelement()


      valid_loss += loss_valid.item() / num_batch_valid
      valid_loss_WM += loss_WM_valid.item() / num_batch_valid
      valid_loss_tot += loss_tot_valid.item() / num_batch_valid
      valid_SDR += SDR_valid
      valid_SNR += SNR_valid


  # Almacenamos las pérdidas del bucle de validacion en cada epoch
  valid_loss_vec[epoch] = valid_loss
  valid_loss_WM_vec[epoch] = valid_loss_WM
  valid_loss_tot_vec[epoch] = valid_loss_tot
  ber_valid[epoch] = incorrect_valid/total_valid
  valid_acc[epoch] = 1-ber_valid[epoch]
  sdr_valid[epoch] = valid_SDR / num_batch_valid
  snr_valid[epoch] = valid_SNR / num_batch_valid

  print_line_v = "Epoch: [%2d] time: %4.4f, loss_autoencoder valid: %.4f" % (epoch + 1, time.time() - start_time, valid_loss)
  print(print_line_v)

  print_line_WM_v= "Epoch: [%2d] time: %4.4f, loss_WM valid: %.4f" % (epoch + 1, time.time() - start_time, valid_loss_WM)
  print(print_line_WM_v)

  print_line_acc_v = "Epoch: [%2d] time: %4.4f, BER: %.4f, accuracy: %.4f" % (epoch + 1, time.time() - start_time, ber_valid[epoch],valid_acc[epoch])
  print(print_line_acc_v)

  print_line_SDR_v = "Epoch: [%2d] time: %4.4f, SDR: %.4f dB" % (epoch + 1, time.time() - start_time ,sdr_valid[epoch])
  print(print_line_SDR_v)

  print_line_SNR_v = "Epoch: [%2d] time: %4.4f, SNR: %.4f dB" % (epoch + 1, time.time() - start_time ,snr_valid[epoch])
  print(print_line_SNR_v)




  # Implementamos un mecanismo de Early-Stopping en el que si no mejoramos el modelo reseteamos

  save_models = (((ber_valid[epoch]< best_BER + 0.05) and (sdr_valid[epoch] > old_sdr)) or ((ber_valid[epoch]< best_BER) and (sdr_valid[epoch] > old_sdr - 1)))

  # GUARDADO PRIMER MODELO
  if save_models :
    # Si hemos mejorado el entrenamiento, reseteamos el contador de strikes y guardamos el modelo
    print("Condiciones MODELO")
    print_line = "     Valid: time: %4.4f, valid_loss: %.4f" % (time.time() - start_time, valid_loss)
    print(print_line)

    strike = 0
    # Save the model
    print("[*] Saving model epoch %d..." % (epoch + 1))
    state = {'epoch': epoch + 1, 'state_dict_embedder': estimator.state_dict() ,'optimizer': optimizer.state_dict(),
                     'train_loss': avg_loss, 'eval_loss': valid_loss}

    torch.save(state, save_net)


    # Si mejoramos el entrenamiento del detector, reseteamos el contador de strikes y guardamos el modelo
    print("Best BER observed in epoch %d..." % (epoch+1))
    print_line_WM = "     Valid: time: %4.4f, valid_loss_WM: %.4f" % (time.time() - start_time, valid_loss_WM)
    print(print_line_WM)

    #
    print("[*] Saving model epoch %d..." % (epoch + 1))
    state_2 = {'epoch': epoch + 1, 'state_dict_decoder': estimator_2.state_dict() ,'optimizer': optimizer.state_dict(),
                     'train_loss': avg_loss_WM, 'eval_loss': valid_loss_WM}
    torch.save(state_2, save_net_2)

    if (ber_valid[epoch] < best_BER) or (ber_valid[epoch]< best_BER + 0.05):
      best_BER = ber_valid[epoch]

    if (sdr_valid[epoch] > old_sdr) or (sdr_valid[epoch] > old_sdr - 1):
      old_sdr = sdr_valid[epoch]



  else:
   # Si NO hemos mejorado el entrenamiento, incrementamos el contador de strikes
    strike += 1

    print_line = "     Valid: time: %4.4f, valid_loss: %.4f *"  % (time.time() - start_time, valid_loss)
    print_line_2 = "     Valid: time: %4.4f, valid_loss: %.4f *"  % (time.time() - start_time, valid_loss_WM)
    print(print_line)
    print(print_line_2)
    # Tras un cierto numero de strikes (también llamado 'patience'), finalizamos el entrenamiento
    if strike > early_stop:
      break


print("[*] Finish training.")

import matplotlib.pyplot as plt
import numpy as np



fig = plt.figure()  # Establecer el tamaño del gráfico

plt.plot(l_mae, label='MAE')  # Plot de l_mae
plt.plot(l_pesq, label='PESQ')  # Plot de l_pesq
plt.plot(l_tot, label='Total')  # Plot de l_tot

# Añadir título y etiquetas
plt.title('Comparación de Losses')
plt.xlabel('Epoch')
plt.ylabel('Valor de la Loss')

# Añadir leyenda
plt.legend()

# Mostrar el gráfico
plt.grid(True)  # Añade una grilla para mejor visualización
plt.show()

fig.savefig('/content/gdrive/MyDrive/MISCOLABS/BBDD/Plots/ComparativaLossesCap3.png')

import matplotlib.pyplot as plt
import numpy as np

# Guardamos los vectores de perdidas y accuracy
# Guardamos los vectores de perdidas y accuracy
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/loss_autoencoder_30_pesq_nv.npy', loss_vector)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/loss_decoder_30_pesq_nv.npy', loss_vector_WM)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/loss_tot_30_pesq_nv.npy', loss_vector_tot)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/avg_loss_autoencoder_30_pesq_nv.npy', avg_loss_vec)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/avg_loss_decoder_30_pesq_nv.npy', avg_loss_WM_vec)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/avg_loss_tot_30_pesq_nv.npy', avg_loss_tot_vec)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/valid_loss_30_pesq_nv.npy', valid_loss_vec)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/valid_loss_WM_30_pesq_nv.npy', valid_loss_WM_vec)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/valid_loss_tot_30_pesq_nv.npy', valid_loss_tot_vec)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/accuracy_30_pesq_nv.npy', epoch_accuracy)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/ber_30_pesq_nv.npy', epoch_ber)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/SNR_30_pesq_nv.npy', snr_vec)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/snr_30_valid_pesq_nv.npy', snr_valid)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/ber_30_valid_pesq_nv.npy', ber_valid)
np.save('/content/gdrive/MyDrive/MISCOLABS/BBDD/Losses&Accuracy/acc_30_valid_pesq_nv.npy', valid_acc)


fig,ax = plt.subplots(3,3,figsize=(10,6),sharex=True)

ax[0][0].plot(loss_vector, color='blue')
ax[0][0].set_title('Loss Autoencoder')
ax[0][0].set_ylabel('loss')
ax[0][0].grid(True)
ax[0][0].legend()

ax[0][1].plot(loss_vector_WM, color='red')
ax[0][1].set_title('Loss Detector')
ax[0][1].set_ylabel('loss')
ax[0][1].grid(True)
ax[0][1].legend()

ax[0][2].plot(loss_vector_tot, color='black')
ax[0][2].set_title('Loss Total')
ax[0][2].set_ylabel('loss')
ax[0][2].grid(True)
ax[0][2].legend()


ax[1][0].plot(avg_loss_vec, color='blue')
ax[1][0].set_title('Loss Autoencoder cum')
ax[1][0].set_ylabel('loss')
ax[1][0].grid(True)
ax[1][0].legend()

ax[1][1].plot(avg_loss_WM_vec, color='red')
ax[1][1].set_title('Loss Detector cum')
ax[1][1].set_ylabel('loss')
ax[1][1].grid(True)
ax[1][1].legend()

ax[1][2].plot(avg_loss_tot_vec, color='black')
ax[1][2].set_title('Loss Total cum')
ax[1][2].set_ylabel('loss')
ax[1][2].grid(True)
ax[1][2].legend()

ax[2][0].plot(valid_loss_vec, color='blue')
ax[2][0].set_title('Valid loss Autoencoder')
ax[2][0].set_xlabel('Epochs')
ax[2][0].set_ylabel('loss')
ax[2][0].grid(True)
ax[2][0].legend()

ax[2][1].plot(valid_loss_WM_vec, color='red')
ax[2][1].set_title('Valid loss Detector')
ax[2][1].set_xlabel('Epochs')
ax[2][1].set_ylabel('loss')
ax[2][1].grid(True)
ax[2][1].legend()

ax[2][2].plot(valid_loss_tot_vec, color='black')
ax[2][2].set_title('Valid loss total')
ax[2][2].set_xlabel('Epochs')
ax[2][2].set_ylabel('loss')
ax[2][2].grid(True)
ax[2][2].legend()

#fig.savefig('/content/gdrive/MyDrive/MISCOLABS/BBDD/Plots/perdidastatq35_steps_v2.png')


plt.figure()
plt.plot(epoch_accuracy)
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy del Decoder')
plt.grid(True)

plt.figure()
plt.plot(valid_acc)
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy del Decoder')
plt.grid(True)


plt.figure()
plt.plot(sdr_vec)
plt.xlabel('Epochs')
plt.ylabel('SDR')
plt.title('SDR')
plt.grid(True)

plt.figure()
plt.plot(sdr_valid)
plt.xlabel('Epochs')
plt.ylabel('SDR')
plt.title('SDR valid')
plt.grid(True)


plt.figure()
plt.plot(avg_loss_vec)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Encoder')
plt.grid(True)

plt.figure()
plt.plot(ber_valid)
plt.xlabel('Epochs')
plt.ylabel('BER')
plt.title('Bit Error Rate Valid')
plt.grid(True)

!cp /content/modelo_e_30_pesq_nv.pth.tar /content/gdrive/MyDrive/MISCOLABS/MODELOS
!cp /content/modelo_d_30_pesq_nv.pth.tar /content/gdrive/MyDrive/MISCOLABS/MODELOS

!pip install pesq
!pip install pystoi

## 3.- Bucle de test. En el bucle de test se comprueba si
## la red neuronal entrenada cumple con su cometido. En este caso
## el cometido principal es obtener a la salida del autoencoder lo
## mismo que en la entrada (aproximadamente)
import torch
import os
import numpy as np
from torch.utils.data import DataLoader
from random import randint
from pesq import pesq
import sys

MODULE_FULL_PATH = '/content/gdrive/MyDrive/MISCOLABS/Bibliotecas'
sys.path.append('/content/gdrive/MyDrive/MISCOLABS/Bibliotecas')

from sdr import sdr
from speech_distortion import speech_distortion_index
from pystoi import stoi

coeficiente_test = 20


# GPU device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# Declaramos el dataset de 'test'
clean_set_test = CleanDataset('/tmp/clean/', conj='test',)

mensaje = np.load('/content/gdrive/MyDrive/MISCOLABS/message_pool.npy')
model_load = '/content/gdrive/MyDrive/MISCOLABS/MODELOS/modelo_e_30_pesq_temp_nv.pth.tar'
model_load_2 = '/content/gdrive/MyDrive/MISCOLABS/MODELOS/modelo_d_30_pesq_temp_nv.pth.tar'

print("Loading model in " + model_load)
print("Loading model in " + model_load_2)
checkpoint = torch.load(model_load)
checkpoint_2 = torch.load(model_load_2)
estimator.load_state_dict(checkpoint['state_dict_embedder'])
estimator_2.load_state_dict(checkpoint_2['state_dict_decoder'])

# Pasamos modelo a la GPU
estimator.to(device)
estimator_2.to(device)
estimator.eval()
estimator_2.eval()

test_dataloader = DataLoader(clean_set_test, batch_size=1, shuffle=True, num_workers=5, collate_fn = custom_collate_fn_test)

SNR = np.zeros(len(test_dataloader))
SDR = np.zeros(len(test_dataloader))
pesq_ratio = np.zeros(len(test_dataloader))
speech_dist_ratio = np.zeros(len(test_dataloader))
stoi_ratio = np.zeros(len(test_dataloader))
BER = np.zeros(len(test_dataloader))

# Empezamos la fase de test

for id,batch in enumerate(test_dataloader):

  inputSTFT = batch
  id_c = chr(id)

  # Generamos marca de agua

  ind = randint(0, len(mensaje)-1)
  msg = np.broadcast_to(mensaje[ind], (10, num_bits))
  temp = np.empty((10,16,2,num_bits))
  temp[:,:,:,:] = np.expand_dims(msg,axis=(1,2))
  input_Message = torch.tensor(temp).float()
  input_Message = input_Message.permute(0,3,1,2)


  # Mandamos tensores a la GPU
  inputSTFT = [tensor.to(device) for tensor in inputSTFT]
  input_Message = input_Message.to(device)
  signals_fin = []
  tensor_comp = []


  with torch.no_grad():

    # Calculamos la salida del autoencoder
    for tensor in inputSTFT:
      tensor = tensor.permute(0,2,1,3)
      batch = tensor
      output = estimator(tensor,input_Message)
      output_atacada = lowpass_attack(output,coeficiente_test)
      output_atacada = additivenoise_attack(output_atacada,coeficiente_test,noise_strength)
      output_atacada = samplesupresion_attack(output_atacada,coeficiente_test,output.size(0),num_samples)
      WM_reconstructed_test = estimator_2(output_atacada)
      WM_reconstructed_test = WM_reconstructed_test.unsqueeze(-1).unsqueeze(-1)
      WM_reconstructed_test = WM_reconstructed_test.repeat(1,1,16,2)

      output_real = output[:,0,:,:]
      output_imag = output[:,1,:,:]

      tensor_real = tensor[:,0,:,:]
      tensor_imag = tensor[:,1,:,:]

      output_atq = torch.complex(output_real,output_imag)
      tensor = torch.complex(tensor_real,tensor_imag)
      output_atq = output_atq.cpu().detach()
      tensor = tensor.cpu().detach()
      signal = torch.istft(output_atq,n_fft=num_fft,hop_length=511,win_length=1023,window=window,normalized=True)
      tensor = torch.istft(tensor,n_fft=num_fft,hop_length=511,win_length=1023,window=window,normalized=True)
      signals_fin.append(signal)
      tensor_comp.append(tensor)

    max_length = max(v.size(1) for v in signals_fin)
    max_length_ten = max(t.size(1) for t in tensor_comp)

    padded_vectors = [F.pad(v, (0, max_length - v.size(1))) for v in signals_fin]
    padded_tensors = [F.pad(t, (0, max_length_ten - t.size(1))) for t in tensor_comp]
    complete_signal = torch.cat(padded_vectors, dim=0)
    complete_signal_or = torch.cat(padded_tensors,dim=0)


    complete_signal = complete_signal.numpy().flatten()
    complete_signal_or = complete_signal_or.numpy().flatten()

    WM_reconstructed_test = umbralizacion(WM_reconstructed_test,0.5)

    incorrect = (WM_reconstructed_test[0,:,0,0] != input_Message[0,:,0,0]).float().sum().item()
    total = WM_reconstructed_test[0,:,0,0].nelement()

    complete_signal_or_norm = complete_signal_or/max(complete_signal_or)
    complete_signal_norm = complete_signal/max(complete_signal)



    SDR[id] = sdr(complete_signal_or,complete_signal)
    pesq_ratio[id] = pesq(fs,complete_signal_or,complete_signal,'wb')
    SNR[id] = snr(complete_signal_or_norm,complete_signal_norm)
    stoi_ratio[id] = stoi(complete_signal_or,complete_signal,16000,extended=False)
    BER[id] = incorrect/total



    # Saving file
    file_enh = '/content/gdrive/MyDrive/MISCOLABS/BBDD/Resultados/PRUEBASCRUZADAS/PESQTEMPRANO' + '/' + 'audio_modelo_30_20deg_' + str(id) + '.wav'
    print("Guardando ...", file_enh)

    wavfile.write(file_enh,16000,complete_signal.astype('int16'))

print("[*] Finish test.")

import matplotlib.pyplot as plt
import numpy as np


batch_r = batch[:,0,:,:]
batch_i = batch[:,1,:,:]
batch_rec = torch.complex(batch_r,batch_i)

batch_rec = batch_rec.cpu().detach()

signal_or = torch.istft(batch_rec,n_fft=num_fft,hop_length=511,win_length=1023,window=window,normalized=True)
signal_or = signal_or.numpy().flatten()
print("señal =", signal_or.shape)

print(signal_or.shape)

print(np.max(np.abs(signal_or)))
print(np.max(np.abs(signal).numpy().flatten()))

t = np.linspace(0,2.044,32194)

signal_max = np.max(np.abs(signal.numpy().flatten()))
signal_norm = signal/signal_max
signal_max_or = np.max(np.abs(signal_or))
signal_norm_or = signal_or/signal_max_or

signal_norm = signal_norm.numpy().flatten()
signal_norm = signal_norm[0:32194]
signal_norm_or = signal_norm_or[0:32194]

print("señal rec = ",signal_norm.shape)
print("señal or = ",signal_norm_or.shape)

dif_signals = signal_norm_or - signal_norm

# Creación de subplots para las señales
fig, axs = plt.subplots(3, 1, figsize=(10, 6), sharex=True)



# Señal 1 en el primer subplot
axs[0].plot(t, signal_norm, color='blue')
axs[0].set_title('Señal de Audio Reconstruida')
axs[0].set_ylabel('Amplitud')
axs[0].grid(True)
axs[0].legend()


# Señal 2 en el segundo subplot
axs[1].plot(t, signal_norm_or, color='red')
axs[1].set_title('Señal de Audio Original')
axs[1].set_ylabel('Amplitud')
axs[1].grid(True)
axs[1].legend()


# Señal 3 en el segundo subplot
axs[2].plot(t, dif_signals, color='black')
axs[2].set_title('Señal de Audio Diferencia')
axs[2].set_xlabel('Tiempo (s)')
axs[2].set_ylabel('Amplitud')
axs[2].grid(True)
axs[2].legend()

fig.savefig('/content/gdrive/MyDrive/MISCOLABS/BBDD/Plots/audios_STEMP.png')

fig_1, ax_1 = plt.subplots()
plt.xlim(0.263,0.267)

ax_1.plot(t,signal_norm,label='Reconstruida',color='blue')
ax_1.plot(t,signal_norm_or, label='Original',color='red')
ax_1.plot(t,dif_signals , label='Distorsión',color='black')
ax_1.grid(True)
ax_1.set_xlabel('Tiempo (s)')
ax_1.set_ylabel('Amplitud')
ax_1.legend()
fig_1.savefig('/content/gdrive/MyDrive/MISCOLABS/BBDD/Plots/zoom_rec_STEMP.png')

audios = np.arange(0,361)
fig_2 , ax_2= plt.subplots()

ax_2.bar(audios,SNR,color='blue')
ax_2.grid(True)
ax_2.set_ylabel('SNR [dB]')
ax_2.set_xlabel('Señales De Audio')
ax_2.set_title('Signal-to-Noise Ratio fase de test')

#fig_2.savefig('/content/gdrive/MyDrive/MISCOLABS/BBDD/Plots/SNR_15_signals_steps.png')

fig_3 , ax_3= plt.subplots()
ax_3.bar(audios,pesq_ratio,color='blue')
ax_3.grid(True)
ax_3.set_ylabel('PESQ [dB]')
ax_3.set_xlabel('Señales De Audio')
ax_3.set_title('PESQ Ratio fase de test')

#fig_3.savefig('/content/gdrive/MyDrive/MISCOLABS/BBDD/Plots/PESQ_15_signals_plot.png')

print('BER = ',np.mean(BER))
print('SNR =', np.mean(SNR))
print('SDR =', np.mean(SDR))
print('PESQ = ',np.mean(pesq_ratio))
print('STOI =',np.mean(stoi_ratio))

import matplotlib.pyplot as plt



WM_recons = WM_reconstructed_test[0,:,0,0]
input_Message_recons = input_Message[0,:,0,0]
print(WM_recons.shape)
print(input_Message_recons.shape)

WM_recons = WM_recons.cpu().detach().numpy()

input_Message_recons = input_Message_recons.cpu().numpy()

for i in range(len(WM_recons)):
  if WM_recons[i] <= 0.5:
      WM_recons[i] = 0
  else:
      WM_recons[i] = 1


incorrect = (WM_recons != input_Message_recons).sum()
print(incorrect)



fig_3, ax =  plt.subplots(2, 1, figsize=(10, 6), sharex=True)
plt.xlim(0,100)



ax[0].plot(WM_recons,label='WM detector',color='blue',drawstyle='steps-post')
ax[0].set_title('Watermark Reconstructed')
ax[0].set_xlabel('Muestras')
ax[0].set_ylabel('Amplitud')
ax[0].grid(True)
ax[0].legend()

ax[1].plot(input_Message_recons,label='WM original',color='red',drawstyle='steps-post')
ax[1].set_title('Watermark Original')
ax[1].set_xlabel('Muestras')
ax[1].set_ylabel('Amplitud')
ax[1].grid(True)
ax[1].legend()

fig_3.savefig('/content/gdrive/MyDrive/MISCOLABS/BBDD/Plots/WM_comparativa_STEMP.png')